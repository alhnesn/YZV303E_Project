{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPp1VQgYRqGUHhDHiyj52Oh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 0- Setup and Imports"],"metadata":{"id":"pToji-R3s3if"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import time\n","import numpy as np\n","from PIL import Image\n","from pathlib import Path\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from skimage.color import rgb2lab, lab2rgb\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from fastai.data.external import untar_data, URLs\n","from fastai.vision.learner import create_body\n","from torchvision.models.resnet import resnet18\n","from fastai.vision.models.unet import DynamicUnet\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDF-CZ4Ds7Hn","executionInfo":{"status":"ok","timestamp":1736055723474,"user_tz":-180,"elapsed":6986,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"17cb7c31-9722-4824-fdc4-135b17bcd688"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using device: cuda\n"]}]},{"cell_type":"markdown","source":["## 1- Load the pretrained GAN model (with ResUnet generator)"],"metadata":{"id":"pQI1kQVbtDTB"}},{"cell_type":"markdown","source":["###### define the classes needed for the GAN model (imports between files are messy in google colab so I just copied and pasted the definitions)"],"metadata":{"id":"JGcqfNQhMD6U"}},{"cell_type":"code","source":["def initialize_weights(model, init_type='normal', gain=0.02):\n","    \"\"\"\n","    General weight initialization function for PyTorch models.\n","\n","    Parameters:\n","        model (nn.Module): The PyTorch model to initialize.\n","        init_type (str): Initialization type ('normal', 'xavier', 'kaiming', 'orthogonal').\n","        gain (float): Gain value for certain initialization methods (e.g., Xavier, Kaiming).\n","\n","    Returns:\n","        nn.Module: The model with initialized weights.\n","    \"\"\"\n","    def init_func(m):\n","        classname = m.__class__.__name__\n","        # Initialize convolutional and linear layers\n","        if hasattr(m, 'weight') and any(layer in classname for layer in ['Conv', 'Linear']):\n","            if init_type == 'normal':\n","                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n","            elif init_type == 'xavier':\n","                nn.init.xavier_normal_(m.weight.data, gain=gain)\n","            elif init_type == 'kaiming':\n","                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in', nonlinearity='leaky_relu')\n","            elif init_type == 'orthogonal':\n","                nn.init.orthogonal_(m.weight.data, gain=gain)\n","            else:\n","                raise ValueError(f\"Unsupported initialization type: {init_type}\")\n","\n","            # Initialize biases to zero if they exist\n","            if hasattr(m, 'bias') and m.bias is not None:\n","                nn.init.constant_(m.bias.data, 0.0)\n","\n","        # Initialize BatchNorm layers\n","        elif 'BatchNorm' in classname:\n","            nn.init.normal_(m.weight.data, mean=1.0, std=gain)\n","            nn.init.constant_(m.bias.data, 0.0)\n","\n","    print(f\"Initializing model weights with {init_type} initialization\")\n","    model.apply(init_func)\n","    return model\n","\n","def initialize_model(model, device):\n","    model = model.to(device)\n","    model = initialize_weights(model)\n","    return model\n","\n","class GANModel(nn.Module):\n","    def __init__(self, net_G, lr_G=2e-4, lr_D=2e-4,\n","                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n","        super().__init__()\n","\n","        # Device setup for GPU/CPU\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.lambda_L1 = lambda_L1  # Weight for L1 loss to balance with adversarial loss (hyperparameter)\n","\n","        # Initialize generator and discriminator models\n","        self.net_G = net_G.to(self.device)\n","        self.net_D = initialize_model(PatchDiscriminator(input_channels=3, num_downsampling=3, num_filters=64), self.device)\n","\n","        # GAN loss (BCE loss)\n","        self.GANcriterion = GANLoss().to(self.device)\n","        self.L1criterion = nn.L1Loss()  # L1 loss for pixel-level accuracy\n","\n","        # Optimizers for generator and discriminator\n","        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n","        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n","\n","    def set_requires_grad(self, model, requires_grad=True):\n","        \"\"\"\n","        Enable or disable gradient computation for a given model.\n","        This is used to freeze/unfreeze a model during training.\n","        \"\"\"\n","        for p in model.parameters():\n","            p.requires_grad = requires_grad\n","\n","    def setup_input(self, data):\n","        \"\"\"\n","        Prepare the input data (L-channel and ab channels)\n","        \"\"\"\n","        self.L = data['L'].to(self.device)  # Grayscale input (L-channel)\n","        self.ab = data['ab'].to(self.device)  # Ground truth color channels (ab)\n","\n","    def forward(self):\n","        \"\"\"\n","        Forward pass through the generator to create fake color (ab channels).\n","        \"\"\"\n","        self.fake_color = self.net_G(self.L)\n","\n","    def backward_D(self):\n","        \"\"\"\n","        Backward pass for the discriminator:\n","        - Classifies real images as real.\n","        - Classifies fake images (from generator) as fake.\n","        \"\"\"\n","        # Combine L-channel and generated color to form a fake image\n","        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n","\n","        # Use .detach() to ensure gradients are not calculated for the generator\n","        fake_preds = self.net_D(fake_image.detach())\n","        self.loss_D_fake = self.GANcriterion(fake_preds, False)  # Loss for fake images\n","\n","        # Combine L-channel and real color to form a real image\n","        real_image = torch.cat([self.L, self.ab], dim=1)\n","        real_preds = self.net_D(real_image)\n","        self.loss_D_real = self.GANcriterion(real_preds, True)  # Loss for real images\n","\n","        # Average the losses and compute gradients\n","        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n","        self.loss_D.backward()  # Backpropagate discriminator loss\n","\n","    def backward_G(self):\n","        \"\"\"\n","        Backward pass for the generator:\n","        - Fool the discriminator (adversarial loss).\n","        - Minimize L1 loss between fake color and ground truth color (pixel-wise accuracy).\n","        \"\"\"\n","        # Combine L-channel and generated color to form a fake image\n","        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n","\n","        # Evaluate the fake image with the discriminator\n","        fake_preds = self.net_D(fake_image)\n","        self.loss_G_GAN = self.GANcriterion(fake_preds, True)  # Generator's adversarial loss\n","\n","        # L1 loss for pixel-level accuracy, scaled by lambda_L1\n","        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n","\n","        # Combine adversarial loss and L1 loss\n","        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n","        self.loss_G.backward()  # Backpropagate generator loss\n","\n","    def optimize(self):\n","        \"\"\"\n","        Optimization step for both generator and discriminator\n","        \"\"\"\n","        # Forward pass and generate fake images\n","        self.forward()\n","\n","        # === Train Discriminator ===\n","        self.net_D.train()\n","        self.set_requires_grad(self.net_D, True)  # Enable gradient computation for discriminator\n","        self.opt_D.zero_grad()\n","        self.backward_D()  # Compute discriminator loss\n","        self.opt_D.step()  # Update discriminator parameters\n","\n","        # === Train Generator ===\n","        self.net_G.train()\n","        self.set_requires_grad(self.net_D, False)  # Freeze discriminator during generator update\n","        self.opt_G.zero_grad()\n","        self.backward_G()  # Compute generator loss\n","        self.opt_G.step()  # Update generator parameters\n","\n","\n","class GANLoss(nn.Module):\n","    def __init__(self, real_label=1.0, fake_label=0.0):\n","        super().__init__()\n","        self.register_buffer('real_label', torch.tensor(real_label))\n","        self.register_buffer('fake_label', torch.tensor(fake_label))\n","\n","        self.loss = nn.BCEWithLogitsLoss()\n","\n","    def get_labels(self, preds, target_is_real):\n","        if target_is_real:\n","            labels = self.real_label\n","        else:\n","            labels = self.fake_label\n","        return labels.expand_as(preds) # tensor ful of 1 or 0s\n","\n","    def __call__(self, preds, target_is_real):\n","        labels = self.get_labels(preds, target_is_real)\n","        loss = self.loss(preds, labels)\n","        return loss\n","\n","class PatchDiscriminator(nn.Module):\n","    \"\"\"\n","    PatchGAN-like discriminator with a configurable number of layers, filters, and strides.\n","    Takes an input with `input_channels` channels and outputs a probability map\n","    indicating real/fake for each patch.\n","    \"\"\"\n","    def __init__(self, input_channels, num_filters=64, num_downsampling=3):\n","        \"\"\"\n","        Args:\n","            input_channels (int): Number of input channels (e.g., 3 for RGB or 1 for grayscale).\n","            num_filters (int): Number of filters for the first convolutional layer.\n","            num_downsampling (int): Number of downsampling steps in the discriminator.\n","        \"\"\"\n","        super().__init__()\n","\n","        # Initial layer: no normalization for the input layer\n","        layers = [\n","            self._conv_block(input_channels, num_filters, normalize=False) # input image\n","        ]\n","\n","        # Downsampling layers\n","        for i in range(num_downsampling):\n","            in_channels = num_filters * 2**i\n","            out_channels = num_filters * 2**(i + 1)\n","\n","            # Use stride=1 for the last downsampling layer\n","            stride = 1 if i == (num_downsampling - 1) else 2\n","            layers.append(self._conv_block(in_channels, out_channels, stride=stride))\n","\n","        # Final layer: outputs a single-channel probability map (logits)\n","        layers.append(\n","            self._conv_block(out_channels, 1, normalize=False, activation=False) # no activation because activation is in the loss\n","        )\n","\n","        # Combine all layers into a sequential model\n","        self.model = nn.Sequential(*layers)\n","\n","    def _conv_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, normalize=True, activation=True):\n","        \"\"\"\n","        Creates a single convolutional block with optional normalization and activation.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            out_channels (int): Number of output channels.\n","            kernel_size (int): Convolution kernel size (default: 4).\n","            stride (int): Stride for the convolution (default: 2).\n","            padding (int): Padding for the convolution (default: 1).\n","            normalize (bool): Whether to use BatchNorm (default: True).\n","            activation (bool): Whether to use LeakyReLU activation (default: True).\n","\n","        Returns:\n","            nn.Sequential: A sequential block containing the specified layers.\n","        \"\"\"\n","        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=not normalize)]\n","        if normalize:\n","            layers.append(nn.BatchNorm2d(out_channels))\n","        if activation:\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass for the discriminator.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (batch_size, input_channels, height, width).\n","\n","        Returns:\n","            torch.Tensor: Output tensor (probability map).\n","        \"\"\"\n","        return self.model(x)"],"metadata":{"id":"zvqkIPk-LcQZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### actually load the pretrained GAN"],"metadata":{"id":"pBEvk9gIMT7w"}},{"cell_type":"code","source":["def load_entire_pretrained_gan(path, device='cuda'):\n","    \"\"\"\n","    Loads the entire model (generator + discriminator + state) from a .pt file.\n","    \"\"\"\n","    if not os.path.exists(path):\n","        raise FileNotFoundError(f\"Pretrained model not found at {path}\")\n","    model = torch.load(path, map_location=device)\n","    print(f\"Loaded pretrained GAN from {path}\")\n","    return model\n","\n","# Path to your previously-trained entire GAN\n","PRETRAINED_MODEL_PATH = \"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/GAN_w_ResUnet/gan_colorizer.pt\"\n","pretrained_gan = load_entire_pretrained_gan(PRETRAINED_MODEL_PATH, device=device)\n","pretrained_gan.eval()\n","\n","\"\"\"\n","  The loaded object 'pretrained_gan' has:\n","  pretrained_gan.net_G  (the generator)\n","  pretrained_gan.net_D  (the discriminator)\n","  .opt_G, .opt_D, etc.\n","\n","But the old net_G was for a single channel in (L),\n","and the old net_D was for 3 channels in (L + ab).\n","We want to adapt them to user hints:\n","  - G: 4 channels in (L, ab_hints, hint_mask)\n","  - D: 6 channels in (L, ab, ab_hints, hint_mask)\n","We'll copy the pretrained weights for the original channels,\n","and initialize the new ones with e.g. Xavier normal.\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":147},"id":"H4oTv27TtSPg","executionInfo":{"status":"ok","timestamp":1736055724555,"user_tz":-180,"elapsed":1083,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"ec8e5c31-c43d-4230-ef9d-fb2740b7db06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-ffe78a9fe133>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(path, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained GAN from /content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/GAN_w_ResUnet/gan_colorizer.pt\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\n  The loaded object 'pretrained_gan' has:\\n  pretrained_gan.net_G  (the generator)\\n  pretrained_gan.net_D  (the discriminator)\\n  .opt_G, .opt_D, etc.\\n\\nBut the old net_G was for a single channel in (L),\\nand the old net_D was for 3 channels in (L + ab).\\nWe want to adapt them to user hints:\\n  - G: 4 channels in (L, ab_hints, hint_mask)\\n  - D: 6 channels in (L, ab, ab_hints, hint_mask)\\nWe'll copy the pretrained weights for the original channels,\\nand initialize the new ones with e.g. Xavier normal.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## Modify pretrained net_G and net_D"],"metadata":{"id":"-TU5DquODx0I"}},{"cell_type":"code","source":["class HintAttentionBlock(nn.Module):\n","    \"\"\"\n","    A small module that:\n","      - Takes as input some feature map F of shape (N, C, H, W)\n","      - Takes the user hint mask M of shape (N, 1, H, W)  [or possibly (N, 2, H, W) if you also want ab info]\n","      - Produces an attention map A of shape (N, 1, H, W) in [0..1]\n","      - Merges F and A to produce a new feature map F_att\n","    This is just a toy example. You can expand on it with more layers.\n","    \"\"\"\n","    def __init__(self, in_channels=3, hidden_channels=64):\n","        super().__init__()\n","        # We'll do a tiny 2-layer CNN that merges the feature map's global average + the mask\n","        self.conv1 = nn.Conv2d(\n","            in_channels= in_channels + 1,   # we cat( F, mask ), so total channels = in_channels + 1\n","            out_channels= hidden_channels,\n","            kernel_size= 3,\n","            padding=1\n","        )\n","        self.conv2 = nn.Conv2d(\n","            in_channels= hidden_channels,\n","            out_channels= 1,  # single-channel attention\n","            kernel_size= 1\n","        )\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, F, mask):\n","        \"\"\"\n","        F:   (N,C,H,W)     - feature maps from the UNet encoder or intermediate layer\n","        mask:(N,1,H,W)     - user hint mask in [0,1]\n","        Output:\n","          F_att: (N,C,H,W) - feature maps modulated by attention\n","        \"\"\"\n","        # 1) cat the mask to F => (N, C+1, H, W)\n","        x = torch.cat([F, mask], dim=1)\n","\n","        # 2) pass through small conv\n","        x = nn.functional.relu(self.conv1(x))\n","        x = self.conv2(x)\n","        # 3) produce attention map in [0..1]\n","        A = self.sigmoid(x)  # shape (N,1,H,W)\n","\n","        # 4) multiply feature maps by A\n","        F_att = F * A  # (N,C,H,W)\n","        return F_att\n","\n","\n","class UserHintAttnGenerator(nn.Module):\n","    \"\"\"\n","    A wrapper around your existing net_G (DynamicUnet with 4 channels in).\n","    We'll inject one (or more) HintAttentionBlock(s).\n","    For demonstration, let's place it near the input or after the first conv.\n","    \"\"\"\n","    def __init__(self, base_unet, attn_channels=64):\n","        super().__init__()\n","        self.base_unet = base_unet\n","        # Let's assume we want to intercept the first or second layer of the unet.\n","        # For simplicity, we'll do it \"pre\" or \"post\" the unet.\n","        # E.g., just do it after we form x = (L, ab_hints, mask).\n","        self.attn_block = HintAttentionBlock(in_channels=3, hidden_channels=64)\n","\n","    def forward(self, L, ab_hints, hint_mask):\n","        \"\"\"\n","        L:   (N,1,H,W)\n","        ab_hints:(N,2,H,W)\n","        hint_mask:(N,1,H,W)\n","        We'll cat => x in shape (N,4,H,W), pass it through an attention block,\n","        then feed to base_unet.\n","        \"\"\"\n","        x = torch.cat([L, ab_hints, hint_mask], dim=1)  # (N,4,H,W)\n","        # apply attention block\n","        # We'll treat x as (features=4ch, mask=1ch?), but we only have 4ch total.\n","        # We do want the mask as a separate param, so let's separate them again.\n","        # Actually, let's assume the mask is channel index #3:\n","        # or we can pass them separately:\n","        features = x[:, :3, :, :]  # (N,3,H,W) - the first 3 channels (L + partial ab?)\n","        M        = x[:, 3:4, :, :] # (N,1,H,W)\n","        # If you also want ab_hints to be separate from the mask, you'd do a different slicing.\n","\n","        # We might want the entire 4ch to be \"features\" though. Let's do it that way:\n","        # features = x\n","        # M = hint_mask\n","        # Then the attention block sees x + mask. But we already have mask in channel #3.\n","        # We'll do it like this for clarity:\n","        F_att = self.attn_block(features, M)  # shape (N,3,H,W)\n","        # Now cat the attended features with the rest if needed:\n","        # If we want 4 channels, we might re-cat the mask or ab hints.\n","        # For demonstration, let's just cat the mask back so unet still sees 4 channels:\n","        x_att = torch.cat([F_att, M], dim=1)  # (N, 4, H, W)\n","\n","        # pass to the base unet\n","        fake_ab = self.base_unet(x_att)\n","        return fake_ab"],"metadata":{"id":"b8GlTl3C9Hd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_res_unet_4_in(n_input=4, n_output=2, size=256):\n","    backbone = resnet18(pretrained=True)\n","    # This tells fastai to adapt the first conv from 3 → 4 channels\n","    body = create_body(backbone, n_in=n_input, cut=-2)\n","    # Now create a unet with n_output=2\n","    model = DynamicUnet(body, n_output, (size, size))\n","    return model\n","\n","def expand_discriminator_input(net_D):\n","    \"\"\"\n","    net_D was originally 3 channels in -> 1 channel out (patch).\n","    We want 6 channels in -> 1 out:\n","      L(1) + ab(2) + ab_h(2) + hint_mask(1) = 6.\n","\n","    We'll:\n","      1) find the first Conv2d layer with in_channels=3\n","      2) replace it with in_channels=6\n","      3) copy the old weights to the first 3 channels\n","      4) random init the extra 3 channels\n","    \"\"\"\n","    first_conv = None\n","    for name, module in net_D.named_modules():\n","        if isinstance(module, nn.Conv2d) and module.in_channels == 3:\n","            first_conv = (name, module)\n","            break\n","    if not first_conv:\n","        raise RuntimeError(\"Could not find a Conv2d with in_channels=3 in net_D. Adapt code if needed.\")\n","\n","    name, old_conv = first_conv\n","\n","    out_c = old_conv.out_channels\n","    ksz = old_conv.kernel_size\n","    stride = old_conv.stride\n","    padding = old_conv.padding\n","    dilation = old_conv.dilation\n","    bias_bool = (old_conv.bias is not None)\n","\n","    new_conv = nn.Conv2d(6, out_c, ksz, stride, padding, dilation, bias=bias_bool)\n","\n","    with torch.no_grad():\n","        # copy old weights => new_conv[:, 0:3]\n","        new_conv.weight[:, 0:3] = old_conv.weight\n","        # random init the extra channels\n","        nn.init.xavier_normal_(new_conv.weight[:, 3:], gain=1.0)\n","        if bias_bool:\n","            new_conv.bias.copy_(old_conv.bias)\n","\n","    _replace_module_by_name(net_D, name, new_conv)\n","\n","\n","def _replace_module_by_name(root_module, module_name, new_module):\n","    \"\"\"\n","    Recursively replaces a named module within root_module with new_module.\n","    module_name is something like 'model.0.0' or 'layers.0.0.conv' etc.\n","    \"\"\"\n","    components = module_name.split(\".\")\n","    obj = root_module\n","    for comp in components[:-1]:\n","        obj = getattr(obj, comp)\n","    setattr(obj, components[-1], new_module)"],"metadata":{"id":"eD1N7GpCD2rN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build a fresh 4-in unet, partially load old net_G weights\n","old_net_G = pretrained_gan.net_G\n","old_sd    = old_net_G.state_dict()\n","\n","model_new = build_res_unet_4_in(n_input=4, n_output=2, size=256).to(device)\n","new_sd    = model_new.state_dict()\n","\n","matched_weights = {}\n","for k, v in old_sd.items():\n","    if k in new_sd and v.shape == new_sd[k].shape:\n","        matched_weights[k] = v\n","new_sd.update(matched_weights)\n","model_new.load_state_dict(new_sd)\n","\n","# 2) wrap it in our attention generator\n","attn_gen = UserHintAttnGenerator(model_new, attn_channels=64).to(device)\n","\n","# 3) set pretrained_gan.net_G to this new attention generator\n","pretrained_gan.net_G = attn_gen"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbt8QvHPTMl6","executionInfo":{"status":"ok","timestamp":1736055725541,"user_tz":-180,"elapsed":989,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"ea8392b6-207e-4ab3-e3be-41801b32fd18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["# expand net_D from 3->6 in-ch\n","old_net_D = pretrained_gan.net_D\n","expand_discriminator_input(old_net_D)\n","pretrained_gan.net_D = old_net_D\n","\n","print(\"Modified the pretrained generator to 4 input channels.\")\n","print(\"Modified the pretrained discriminator to 6 input channels.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y13wdjEW-87P","executionInfo":{"status":"ok","timestamp":1736055725542,"user_tz":-180,"elapsed":7,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"2e184ea6-db6c-4432-f421-b40d19bd2bc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Modified the pretrained generator to 4 input channels.\n","Modified the pretrained discriminator to 6 input channels.\n"]}]},{"cell_type":"markdown","source":["## Helper Functions"],"metadata":{"id":"glvxh69aEj1P"}},{"cell_type":"code","source":["def lab_to_rgb(L, ab):\n","    \"\"\"\n","    Convert normalized LAB -> RGB\n","    L in [-1,1] => [0..100]\n","    ab in [-1,1] => [-110..110]\n","    Returns (N,H,W,3) in [0,1] or single image if no batch dimension\n","    \"\"\"\n","    single_image = (L.ndim == 3)  # (1,H,W), (2,H,W)\n","    if single_image:\n","        L = L.unsqueeze(0)\n","        ab = ab.unsqueeze(0)\n","\n","    L_den = (L + 1.) * 50.0\n","    a_den = ab[:, [0], :, :] * 110.0\n","    b_den = ab[:, [1], :, :] * 110.0\n","\n","    Lab = torch.cat([L_den, a_den, b_den], dim=1)  # (N,3,H,W)\n","    Lab = Lab.permute(0,2,3,1).cpu().numpy()\n","\n","    out_list = []\n","    for i in range(Lab.shape[0]):\n","        out_list.append(lab2rgb(Lab[i].astype(np.float64)))\n","    out_array = np.stack(out_list, axis=0)  # (N,H,W,3)\n","    if single_image:\n","        out_array = out_array[0]\n","    return out_array\n","\n","def single_lab_to_rgb(L_val, A_val, B_val):\n","    \"\"\"\n","    Convert a single pixel (L_val, A_val, B_val) from normalized lab -> RGB\n","    Returns np array [r,g,b] in [0..1].\n","    \"\"\"\n","    L_den = (L_val + 1.0) * 50.\n","    A_den = A_val * 110.\n","    B_den = B_val * 110.\n","    patch = np.array([[[L_den, A_den, B_den]]], dtype=np.float64)\n","    rgb_01 = lab2rgb(patch)  # shape (1,1,3) in [0..1]\n","    return rgb_01[0,0]  # shape (3,)\n","\n","def visualize_colorization(model, batch, max_images=5, hint_radius=0):\n","    \"\"\"\n","    Show grayscale + hints in their actual color, predicted color, ground truth.\n","\n","    - We'll \"brighten\" grayscale by denormalizing from [-1,1] to [0,1].\n","    - We'll place a small square or circle with the actual color of the hint pixel.\n","    - If hint_radius>0, we show a slightly bigger area for the hint color.\n","    \"\"\"\n","    model.eval()\n","    with torch.no_grad():\n","        model.setup_input(batch)\n","        model.forward()\n","        fake_ab = model.fake_color.detach().cpu()  # (N,2,H,W)\n","\n","    L      = batch['L'].cpu()      # (N,1,H,W) in [-1,1]\n","    ab_gt  = batch['ab'].cpu()     # (N,2,H,W)\n","    ab_h   = batch['ab_hints'].cpu()   # (N,2,H,W)\n","    mask_h = batch['hint_mask'].cpu()  # (N,1,H,W)\n","\n","    rgb_fake = lab_to_rgb(L, fake_ab)\n","    rgb_real = lab_to_rgb(L, ab_gt)\n","\n","    # Denormalize grayscale to [0..1] for display\n","    L_01 = (L + 1.) * 0.5  # now in [0..1]\n","    L_np = L_01[:, 0, :, :].numpy()\n","\n","    N = L.size(0)\n","    n_show = min(max_images, N)\n","    fig, axs = plt.subplots(n_show, 3, figsize=(12, 4*n_show))\n","\n","    for i in range(n_show):\n","        # 1) Grayscale as 3-ch in [0..1]\n","        gray3 = np.stack([L_np[i]]*3, axis=-1)  # (H,W,3)\n","\n","        # Where mask=1, we place the actual user color from ab_h.\n","        # If the user wanted a bigger \"radius\", we do that in the next lines.\n","        H,W = gray3.shape[:2]\n","        mh = mask_h[i,0].numpy()  # shape (H,W)\n","        ab_h_i = ab_h[i].numpy()  # shape (2,H,W)\n","\n","        # For each pixel where mask=1, we'll convert that ab_h_i to an actual color\n","        # and place it in gray3. Possibly do a local region around that pixel.\n","        coords = np.argwhere(mh>0.5)\n","        for (r,c) in coords:\n","            # Convert ab_h to rgb for that pixel\n","            # We also need the L from L_01\n","            # But note: L_01 is in [0..1], we have to re-convert if we want the same normalization as the net\n","            # Instead, we can do: L_val in [-1..1], ab in [-1..1].\n","            # So let's get the 'original' L from batch, not L_01\n","            L_val = L[i,0,r,c].item()\n","            A_val = ab_h[i,0,r,c].item()\n","            B_val = ab_h[i,1,r,c].item()\n","\n","            # convert single pixel lab -> rgb\n","            color_3 = single_lab_to_rgb(L_val, A_val, B_val)  # [r,g,b] in [0..1]\n","\n","            gray3[r, c] = color_3\n","            # We'll fill a local region of size (2*hint_radius+1)\n","            # for rr in range(r-hint_radius, r+hint_radius+1):\n","            #     for cc in range(c-hint_radius, c+hint_radius+1):\n","            #         if 0 <= rr < H and 0 <= cc < W:\n","            #             gray3[rr, cc] = color_3\n","\n","\n","        ax0 = axs[i,0] if n_show>1 else axs[0]\n","        ax1 = axs[i,1] if n_show>1 else axs[1]\n","        ax2 = axs[i,2] if n_show>1 else axs[2]\n","\n","        ax0.imshow(gray3, vmin=0, vmax=1)\n","        ax0.set_title(\"Grayscale + Hints\")\n","        ax0.axis(\"off\")\n","\n","        ax1.imshow(rgb_fake[i])\n","        ax1.set_title(\"Predicted\")\n","        ax1.axis(\"off\")\n","\n","        ax2.imshow(rgb_real[i])\n","        ax2.set_title(\"Ground Truth\")\n","        ax2.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"6ia8o6snEmHe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define dataset"],"metadata":{"id":"WeNH1XaaEY_a"}},{"cell_type":"code","source":["# We'll use the same COCO sample but we'll incorporate random n_hints in [1..5].\n","# This generally helps the model learn to handle variable number of user hints.\n","\n","IMG_SIZE = 256\n","SEED = 42\n","np.random.seed(SEED)\n","\n","coco_path = untar_data(URLs.COCO_SAMPLE)\n","all_files = list((coco_path / 'train_sample').glob(\"*.jpg\"))\n","np.random.shuffle(all_files)\n","\n","N = min(len(all_files), 10000)\n","all_files = all_files[:N]\n","\n","split_idx = int(0.8 * len(all_files))\n","train_files = all_files[:split_idx]\n","val_files   = all_files[split_idx:]\n","\n","print(f\"Train set size: {len(train_files)}\")\n","print(f\"Val set size:   {len(val_files)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSLgOqtkEsy_","executionInfo":{"status":"ok","timestamp":1736055725542,"user_tz":-180,"elapsed":6,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"ef6027c7-d911-47ea-ecdd-e1fdd4385e5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set size: 8000\n","Val set size:   2000\n"]}]},{"cell_type":"code","source":["class UserHintLABDataset(Dataset):\n","    \"\"\"\n","    - For training set: random hints each time __getitem__ is called\n","    - For validation set: random hints but consistent each epoch (we store them once).\n","    \"\"\"\n","    def __init__(self, paths, split='train', n_hints_min=1, n_hints_max=5, hint_radius=0):\n","        self.paths = paths\n","        self.split = split\n","        self.n_min = n_hints_min\n","        self.n_max = n_hints_max\n","        self.hint_radius = hint_radius\n","\n","        if split=='train':\n","            self.transform = transforms.Compose([\n","                transforms.Resize((IMG_SIZE, IMG_SIZE), Image.BICUBIC),\n","                transforms.RandomHorizontalFlip(),\n","            ])\n","            # For training, we do *not* store hints; we generate them each time\n","            self.stored_hints = [None]*len(paths)\n","        else:\n","            self.transform = transforms.Resize((IMG_SIZE, IMG_SIZE), Image.BICUBIC)\n","            # For validation, we store hints the first time we see the item\n","            self.stored_hints = [None]*len(paths)\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, idx):\n","        path = self.paths[idx]\n","        with Image.open(path).convert(\"RGB\") as img:\n","            img = self.transform(img)\n","\n","        rgb_np = np.array(img)\n","        lab_np = rgb2lab(rgb_np).astype(np.float32)\n","        L_, a_, b_ = lab_np[...,0], lab_np[...,1], lab_np[...,2]\n","        # normalize\n","        L_norm = (L_/50.) - 1.\n","        a_norm = a_/110.\n","        b_norm = b_/110.\n","\n","        L_t  = torch.from_numpy(L_norm).unsqueeze(0)   # (1,H,W)\n","        ab_t = torch.from_numpy(np.stack([a_norm,b_norm],axis=0))  # (2,H,W)\n","\n","        H,W = L_t.shape[1], L_t.shape[2]\n","        ab_hints= np.zeros((2,H,W), dtype=np.float32)\n","        mask    = np.zeros((1,H,W), dtype=np.float32)\n","\n","        # =============== CREATE OR REUSE HINTS ===============\n","        if self.split=='train':\n","            # Generate new random hints each time\n","            n_hints = np.random.randint(self.n_min, self.n_max+1)\n","            coords = []\n","            for _ in range(n_hints):\n","                rr = np.random.randint(0,H)\n","                cc = np.random.randint(0,W)\n","                coords.append((rr, cc))\n","            # optional: store them if you want to see them again, but we do random each time\n","        else:\n","            # Validation\n","            if self.stored_hints[idx] is None:\n","                # generate once\n","                n_hints = np.random.randint(self.n_min, self.n_max+1)\n","                coords = []\n","                for _ in range(n_hints):\n","                    rr = np.random.randint(0,H)\n","                    cc = np.random.randint(0,W)\n","                    coords.append((rr, cc))\n","                self.stored_hints[idx] = coords\n","            else:\n","                coords = self.stored_hints[idx]\n","\n","        # apply them (optionally with a radius effect for \"bigger\" hints)\n","        for (rr,cc) in coords:\n","            for r_ in range(rr-self.hint_radius, rr+self.hint_radius+1):\n","                for c_ in range(cc-self.hint_radius, cc+self.hint_radius+1):\n","                    if 0 <= r_ < H and 0 <= c_ < W:\n","                        ab_hints[0, r_, c_] = a_norm[r_, c_]\n","                        ab_hints[1, r_, c_] = b_norm[r_, c_]\n","                        mask[0, r_, c_] = 1.0\n","\n","        ab_h_t = torch.from_numpy(ab_hints)\n","        mask_t = torch.from_numpy(mask)\n","\n","        return {\n","            'L': L_t,       # (1,H,W)\n","            'ab': ab_t,     # (2,H,W)\n","            'ab_hints': ab_h_t,   # (2,H,W)\n","            'hint_mask': mask_t   # (1,H,W)\n","        }"],"metadata":{"id":"qfGatTBQE0wa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = UserHintLABDataset(train_files, split='train', n_hints_min=1, n_hints_max=5, hint_radius=5)\n","val_ds   = UserHintLABDataset(val_files,   split='val',   n_hints_min=1, n_hints_max=5, hint_radius=5)\n","\n","train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n","val_dl   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=4, pin_memory=True)"],"metadata":{"id":"2EbbU6UqFCLK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modify the GAN model (setup_input and forward)"],"metadata":{"id":"FY8nLvshFIMc"}},{"cell_type":"code","source":["class UserHintGANModel(type(pretrained_gan)):\n","    \"\"\"\n","    We'll dynamically create a subclass so that:\n","      - setup_input handles L, ab, ab_hints, hint_mask\n","      - forward uses net_G(x) with x = cat(L, ab_hints, hint_mask)\n","      - net_D sees cat(L, ab, ab_hints, hint_mask) => 6-ch\n","    \"\"\"\n","    def setup_input(self, data):\n","        self.L = data['L'].to(self.device)\n","        self.ab = data['ab'].to(self.device)\n","        self.ab_hints = data['ab_hints'].to(self.device)\n","        self.hint_mask = data['hint_mask'].to(self.device)\n","\n","    def forward(self):\n","        self.fake_color = self.net_G(self.L, self.ab_hints, self.hint_mask)\n","\n","    def backward_D(self):\n","        # real\n","        real_in = torch.cat([self.L, self.ab, self.ab_hints, self.hint_mask], dim=1)  # (N,6,H,W)\n","        pred_real = self.net_D(real_in)\n","        self.loss_D_real = self.GANcriterion(pred_real, True)\n","\n","        # fake\n","        fake_in = torch.cat([self.L, self.fake_color, self.ab_hints, self.hint_mask], dim=1)\n","        pred_fake= self.net_D(fake_in.detach())\n","        self.loss_D_fake = self.GANcriterion(pred_fake, False)\n","\n","        self.loss_D = 0.5*(self.loss_D_real + self.loss_D_fake)\n","        self.loss_D.backward()\n","\n","    def backward_G(self):\n","        # adv\n","        fake_in = torch.cat([self.L, self.fake_color, self.ab_hints, self.hint_mask], dim=1)\n","        pred_fake= self.net_D(fake_in)\n","        self.loss_G_GAN = self.GANcriterion(pred_fake, True)\n","\n","        # L1\n","        # self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n","\n","        # SmoothL1\n","        criterion = nn.SmoothL1Loss(beta=1.0)\n","        self.loss_G_L1 = criterion(self.fake_color, self.ab) * self.lambda_L1\n","\n","        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n","        self.loss_G.backward()"],"metadata":{"id":"1i4YdpFwFYm9","executionInfo":{"status":"ok","timestamp":1736058074378,"user_tz":-180,"elapsed":219,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["pretrained_gan.__class__ = UserHintGANModel\n","pretrained_gan.device = device\n","pretrained_gan.net_G.to(device)\n","pretrained_gan.net_D.to(device)\n","\n","print(\"Assigned new class to pretrained_gan for user hints. Now we can train it.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPnmQQdFFcDJ","executionInfo":{"status":"ok","timestamp":1736058076836,"user_tz":-180,"elapsed":204,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"52439582-166b-46a7-d1d8-52fe6ea6a3f6"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Assigned new class to pretrained_gan for user hints. Now we can train it.\n"]}]},{"cell_type":"markdown","source":["## Training loop"],"metadata":{"id":"nIgeFQM-Filx"}},{"cell_type":"code","source":["def train_user_guided_gan(\n","    gan_model, train_dl, val_dl=None, epochs=5, lr_G=2e-4, lr_D=2e-4,\n","    beta1=0.5, beta2=0.999, save_path=None, vis_interval=3, hint_radius=2\n","):\n","    \"\"\"\n","    Fine-tune the pretrained GAN with user hints in G and D.\n","    \"\"\"\n","    # Re-setup the optimizers because model arcitecture changes\n","    gan_model.opt_G = optim.Adam(gan_model.net_G.parameters(), lr=lr_G, betas=(beta1,beta2))\n","    gan_model.opt_D = optim.Adam(gan_model.net_D.parameters(), lr=lr_D, betas=(beta1,beta2))\n","\n","    for epoch in range(epochs):\n","        gan_model.train()\n","        epoch_loss_D, epoch_loss_G = 0.0, 0.0\n","\n","        for batch in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{epochs}\"):\n","            gan_model.setup_input(batch)\n","            gan_model.optimize()\n","\n","            epoch_loss_D += gan_model.loss_D.item()\n","            epoch_loss_G += gan_model.loss_G.item()\n","\n","        epoch_loss_D /= len(train_dl)\n","        epoch_loss_G /= len(train_dl)\n","        print(f\"[Epoch {epoch+1}/{epochs}] Loss_D: {epoch_loss_D:.4f}, Loss_G: {epoch_loss_G:.4f}\")\n","\n","        if val_dl:\n","            gan_model.net_G.eval()\n","            val_loss = 0.0\n","            with torch.no_grad():\n","                for val_batch in val_dl:\n","                    gan_model.setup_input(val_batch)\n","                    gan_model.forward()\n","                    val_loss += nn.L1Loss()(gan_model.fake_color, gan_model.ab).item()\n","            val_loss /= len(val_dl)\n","            print(f\"         Val L1: {val_loss:.4f}\")\n","\n","            if (epoch+1) % vis_interval == 0:\n","                sample_batch = next(iter(val_dl))\n","                visualize_colorization(gan_model, sample_batch, max_images=3, hint_radius=hint_radius)\n","\n","    if save_path:\n","        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","        torch.save(gan_model, save_path)\n","        print(f\"Model saved to {save_path}\")"],"metadata":{"id":"T4L2w48OFliI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run fine tuning"],"metadata":{"id":"JYyfZW2xJ9XR"}},{"cell_type":"code","source":["SAVE_PATH = \"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/user_guided/user_hint_gan_finetuned_attention.pt\"\n","\n","train_user_guided_gan(\n","    gan_model=pretrained_gan,\n","    train_dl=train_dl,\n","    val_dl=val_dl,\n","    epochs=20,\n","    vis_interval=3,\n","    save_path=SAVE_PATH,\n","    hint_radius=5\n",")\n","\n","print(\"Fine-tuning complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1lOyJwuaQ5DDqeNDZfyl6whUbQ_J9QMSn"},"id":"WrcWWFpuKC4z","outputId":"439dbea1-2666-498f-ea23-849faf6c810d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Test with user images"],"metadata":{"id":"jOtO3AxlgZDY"}},{"cell_type":"code","source":["def test_user_hinted_colorization(gan_model, input_path, output_dir=None, img_size=256,\n","                                  n_hints=3, hint_radius=2):\n","    \"\"\"\n","    Test the user-hinted colorization model on user-provided images (or a directory of images).\n","    For demonstration, we create random hints for each image.\n","\n","    Parameters:\n","      - gan_model: The user-hinted GAN model (already loaded).\n","      - input_path: Path to an image or directory containing images.\n","      - output_dir: Where to save colorized images (optional).\n","      - img_size: resize for input.\n","      - n_hints: number of random hint points to place per image.\n","      - hint_radius: local radius around each hint pixel to fill with the same color.\n","    \"\"\"\n","    gan_model.eval()\n","\n","    input_path = Path(input_path)\n","    if input_path.is_file():\n","        image_paths = [input_path]\n","    elif input_path.is_dir():\n","        image_paths = sorted(list(input_path.glob(\"*.[pjPJ][pnPN][gG]\")))\n","    else:\n","        raise ValueError(f\"Invalid input path: {input_path}\")\n","\n","    if output_dir:\n","        output_dir = Path(output_dir)\n","        output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # We'll define a transform to produce the L-ab space ourselves\n","    # since we want to create random hints\n","    def to_lab_tensors(img_pil):\n","        # resize\n","        img_pil = img_pil.resize((img_size, img_size), Image.BICUBIC)\n","        rgb_np  = np.array(img_pil)\n","        lab_np  = rgb2lab(rgb_np).astype(np.float32)\n","        L_, a_, b_ = lab_np[...,0], lab_np[...,1], lab_np[...,2]\n","\n","        L_norm = (L_/50.) - 1.\n","        a_norm = a_/110.\n","        b_norm = b_/110.\n","\n","        L_t  = torch.from_numpy(L_norm).unsqueeze(0)   # (1,H,W)\n","        ab_t = torch.from_numpy(np.stack([a_norm,b_norm],axis=0))  # (2,H,W)\n","        return L_t, ab_t\n","\n","    for idx, img_path in enumerate(tqdm(image_paths, desc=\"Processing images\")):\n","        pil_img = Image.open(img_path).convert(\"RGB\")\n","        W,H = pil_img.size\n","\n","        L_t, ab_t = to_lab_tensors(pil_img)  # (1,H,W), (2,H,W)\n","        L_np = L_t.numpy()[0]  # (H,W)\n","        a_np = ab_t.numpy()[0] # (H,W)\n","        b_np = ab_t.numpy()[1] # (H,W)\n","\n","        # create hints\n","        ab_hints = np.zeros_like(ab_t.numpy())  # shape (2,H,W)\n","        mask     = np.zeros((1,)+ab_hints.shape[1:], np.float32) # (1,H,W)\n","        HH,WW = ab_hints.shape[1:]\n","\n","        # random coords\n","        for _ in range(n_hints):\n","            rr = np.random.randint(0,HH)\n","            cc = np.random.randint(0,WW)\n","            # fill the region\n","            for r_ in range(rr-hint_radius, rr+hint_radius+1):\n","                for c_ in range(cc-hint_radius, cc+hint_radius+1):\n","                    if 0 <= r_ < HH and 0 <= c_ < WW:\n","                        ab_hints[0, r_, c_] = a_np[r_, c_]\n","                        ab_hints[1, r_, c_] = b_np[r_, c_]\n","                        mask[0, r_, c_]     = 1.\n","\n","        ab_h_t = torch.from_numpy(ab_hints)\n","        mask_t = torch.from_numpy(mask)\n","\n","        data = {\n","            'L': L_t.unsqueeze(0).to(device),        # (1,1,H,W)\n","            'ab': ab_t.unsqueeze(0).to(device),      # (1,2,H,W)\n","            'ab_hints': ab_h_t.unsqueeze(0).to(device),  # (1,2,H,W)\n","            'hint_mask': mask_t.unsqueeze(0).to(device)  # (1,1,H,W)\n","        }\n","\n","        with torch.no_grad():\n","            gan_model.setup_input(data)\n","            gan_model.forward()\n","            fake_ab = gan_model.fake_color.detach().cpu()  # (1,2,H,W)\n","\n","        # convert to RGB\n","        rgb_fake = lab_to_rgb(data['L'].cpu(), fake_ab)[0]  # (H,W,3)\n","\n","        # Visualization or saving\n","        if idx<5:\n","            # Show\n","            plt.figure(figsize=(12,5))\n","\n","            # 1) Show grayscale + hints\n","            L_01 = (data['L'][0,0].cpu().numpy()+1.)*0.5  # [0..1]\n","            gray3 = np.stack([L_01]*3, axis=-1)\n","            coords = np.argwhere(mask[0]>0.5)\n","            for (r,c) in coords:\n","                # get ab from ab_hints\n","                A_val = ab_hints[0, r, c]\n","                B_val = ab_hints[1, r, c]\n","                L_val = data['L'][0,0,r,c].item()\n","                color_3 = single_lab_to_rgb(L_val, A_val, B_val)\n","                gray3[r, c] = color_3\n","\n","            plt.subplot(1,3,1)\n","            plt.imshow(gray3, vmin=0, vmax=1)\n","            plt.title(\"Gray + Hints\")\n","            plt.axis(\"off\")\n","\n","            # 2) Show predicted\n","            plt.subplot(1,3,2)\n","            plt.imshow(rgb_fake)\n","            plt.title(\"Predicted\")\n","            plt.axis(\"off\")\n","\n","            # 3) Show original\n","            plt.subplot(1,3,3)\n","            plt.imshow(pil_img)\n","            plt.title(\"Original\")\n","            plt.axis(\"off\")\n","\n","            plt.tight_layout()\n","            plt.show()\n","\n","        if output_dir:\n","            out_dir = Path(output_dir)\n","            out_dir.mkdir(parents=True, exist_ok=True)\n","            out_file = out_dir / img_path.name\n","            plt.imsave(out_file, rgb_fake)\n","            print(f\"Saved colorized image to {out_file}\")\n","\n"],"metadata":{"id":"zMliLnoWgczL","executionInfo":{"status":"ok","timestamp":1736057712807,"user_tz":-180,"elapsed":232,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["input_path = \"/content/test_images\"\n","test_user_hinted_colorization(pretrained_gan, input_path, hint_radius=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1SAoYFcoaaQzG0HHLNbq8Rzv33T1nUAvx"},"id":"DMY38805gsue","executionInfo":{"status":"ok","timestamp":1736057781302,"user_tz":-180,"elapsed":4919,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"87bf7fa0-99a4-4b4b-ed0c-3701442f627b"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["##### 20 more epochs"],"metadata":{"id":"ZTX8AlcfIGac"}},{"cell_type":"code","source":["SAVE_PATH = \"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/user_guided/user_hint_gan_finetuned_attention40.pt\"\n","\n","train_user_guided_gan(\n","    gan_model=pretrained_gan,\n","    train_dl=train_dl,\n","    val_dl=val_dl,\n","    epochs=20,\n","    vis_interval=3,\n","    save_path=SAVE_PATH,\n","    hint_radius=5\n",")\n","\n","print(\"Fine-tuning complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1DQvwy3YTNJXMT5vmT3LxKJAdT6OnkXbO"},"id":"Rv60w6LlIIlW","executionInfo":{"status":"ok","timestamp":1736059453292,"user_tz":-180,"elapsed":1317305,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"894eb52d-a5a8-4a99-dbf9-ec475237cbf7"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"101TX8CxIORr"},"execution_count":null,"outputs":[]}]}