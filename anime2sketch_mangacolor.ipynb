{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPQ/6zV6bySCtvof9S44yi/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 1) Setup & Data Preperation"],"metadata":{"id":"ghJGQKxe1rk7"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NAbGYcN1xp_","executionInfo":{"status":"ok","timestamp":1736088399086,"user_tz":-180,"elapsed":3701,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"7cc92685-e15c-44ca-b3d3-82395aa6726f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# prompt: copy from \"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/Anime2Sketch/netG.pth\" to \"/content/weights\"\n","\n","import shutil\n","import os\n","\n","# Create the destination directory if it doesn't exist\n","os.makedirs(\"/content/weights\", exist_ok=True)\n","\n","shutil.copy(\"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/Anime2Sketch/netG.pth\", \"/content/weights\")\n"],"metadata":{"id":"hHSiz6VtM6mH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !cp \"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/data/ColoredMangaMain.zip\" \"/content/\"\n","# !unzip -q \"/content/ColoredMangaMain.zip\" -d \"/content/ColoredMangaMain\"\n","\n","import os\n","import glob\n","import random\n","import functools\n","import numpy as np\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from skimage.color import rgb2lab, lab2rgb\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Locate images inside the unzipped folder.\n","# Adjust this path if your images are in subfolders, etc.\n","all_image_paths = glob.glob(\"/content/ColoredMangaMain/**/*.png\", recursive=True)\n","all_image_paths += glob.glob(\"/content/ColoredMangaMain/**/*.jpg\", recursive=True)\n","all_image_paths += glob.glob(\"/content/ColoredMangaMain/**/*.jpeg\", recursive=True)\n","\n","print(f\"Found {len(all_image_paths)} images.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Ry_PTzt1qik","executionInfo":{"status":"ok","timestamp":1736092761918,"user_tz":-180,"elapsed":416,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"55e7a9df-2030-4b29-e900-1eb17bf14620"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Found 36285 images.\n"]}]},{"cell_type":"code","source":["np.random.seed(42)\n","np.random.shuffle(all_image_paths)\n","N = min(len(all_image_paths), 15000)\n","all_image_paths = all_image_paths[:N]\n","train_size = int(0.8 * len(all_image_paths))\n","train_paths = all_image_paths[:train_size]\n","val_paths   = all_image_paths[train_size:]\n","\n","print(f\"Train set size: {len(train_paths)}\")\n","print(f\"Val set size:   {len(val_paths)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMcA0hYw13Yl","executionInfo":{"status":"ok","timestamp":1736092766773,"user_tz":-180,"elapsed":1011,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"26092592-7739-4c83-93a8-4e041a781354"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set size: 12000\n","Val set size:   3000\n"]}]},{"cell_type":"markdown","source":["## 2) Dataset Definition"],"metadata":{"id":"KRTA5K3Q2Wov"}},{"cell_type":"code","source":["class MangaColorizationDataset(Dataset):\n","    \"\"\"\n","    Dataset for manga colorization.\n","    - Reads a colored image from disk\n","    - Converts to grayscale with CV2 (BGR->GRAY)\n","    - Threshold the grayscale to produce an \"uncolored\" L\n","    - Convert original image to Lab using skimage\n","    - Output: (l_input, l_diff, a, b)\n","    \"\"\"\n","    def __init__(self, image_paths, threshold=100, img_size=256):\n","        self.image_paths = image_paths\n","        self.threshold = threshold\n","        self.img_size = img_size  # We will resize images to (img_size, img_size)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        # Read image with CV2, which is BGR by default\n","        bgr_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        if bgr_img is None:\n","            raise ValueError(f\"Error loading image at: {img_path}\")\n","\n","        # Resize if desired\n","        if self.img_size is not None:\n","            bgr_img = cv2.resize(bgr_img, (self.img_size, self.img_size))\n","\n","        # Convert to grayscale with CV2\n","        gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n","\n","        # Threshold\n","        #   \"If grayscale >= threshold => 255 else grayscale\"\n","        l_input = np.where(gray_img >= self.threshold, 255, gray_img).astype(np.float32)\n","\n","        # The difference between the \"true grayscale\" and the thresholded one\n","        #   true grayscale in range [0..255]\n","        #   l_input in range [0..255]\n","        # => difference in range [-255..+255]\n","        l_difference = (gray_img.astype(np.float32) - l_input).astype(np.float32)\n","\n","        # Convert original BGR->RGB->Lab for the ground truth color\n","        rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n","        rgb_img_float = rgb_img.astype(np.float32) / 255.0  # [0..1]\n","\n","        lab_img = rgb2lab(rgb_img_float)  # returns L:[0..100], a,b:[-128..127]\n","        L_lab = lab_img[...,0]  # in [0..100]\n","        A_lab = lab_img[...,1]  # in [-128..127]\n","        B_lab = lab_img[...,2]  # in [-128..127]\n","\n","        # Normalize the inputs\n","        # l_input in [0..255], => /255 => [0..1]\n","        l_input_norm = l_input / 255.0\n","        # l_difference in [-255..255], => /255 => [-1..1]\n","        l_diff_norm  = l_difference / 255.0\n","\n","        # For A and B, normalize to [-1..+1]\n","        a_norm = A_lab / 128.0\n","        b_norm = B_lab / 128.0\n","\n","        # Convert to tensors, shape: (1, H, W)\n","        t_l_input = torch.tensor(l_input_norm, dtype=torch.float32).unsqueeze(0)\n","        t_l_diff  = torch.tensor(l_diff_norm, dtype=torch.float32).unsqueeze(0)\n","        t_a       = torch.tensor(a_norm,      dtype=torch.float32).unsqueeze(0)\n","        t_b       = torch.tensor(b_norm,      dtype=torch.float32).unsqueeze(0)\n","\n","        return t_l_input, t_l_diff, t_a, t_b"],"metadata":{"id":"0QDFxWox2gy2","executionInfo":{"status":"ok","timestamp":1736092773083,"user_tz":-180,"elapsed":1035,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["#### Create datasets/dataloaders"],"metadata":{"id":"uR6M3ISH2rUN"}},{"cell_type":"code","source":["train_dataset = MangaColorizationDataset(train_paths, threshold=100, img_size=512)\n","val_dataset   = MangaColorizationDataset(val_paths,   threshold=100, img_size=512)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n","val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=4)\n","\n","print(\"Train/Val loader ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpsjWos_2uGP","executionInfo":{"status":"ok","timestamp":1736095384850,"user_tz":-180,"elapsed":571,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"703e4e0f-2338-48f4-c1a5-c92d52e65983"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Val loader ready.\n"]}]},{"cell_type":"markdown","source":["## 3) Model Definition (U-Net)"],"metadata":{"id":"Tud3mNYy25At"}},{"cell_type":"code","source":["class UnetSkipConnectionBlock(nn.Module):\n","    \"\"\"Defines the U-Net submodule with skip connections.\"\"\"\n","    def __init__(self, outer_nc, inner_nc, input_nc=None,\n","                 submodule=None, outermost=False, innermost=False,\n","                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n","        super(UnetSkipConnectionBlock, self).__init__()\n","        self.outermost = outermost\n","        if isinstance(norm_layer, functools.partial):\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        if input_nc is None:\n","            input_nc = outer_nc\n","\n","        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n","                             stride=2, padding=1, bias=use_bias)\n","        downrelu = nn.LeakyReLU(0.2, True)\n","        downnorm = norm_layer(inner_nc)\n","        uprelu = nn.ReLU(True)\n","        upnorm = norm_layer(outer_nc)\n","\n","        if outermost:\n","            upconv = nn.ConvTranspose2d(\n","                inner_nc * 2, outer_nc, kernel_size=4,\n","                stride=2, padding=1\n","            )\n","            down = [downconv]\n","            up = [uprelu, upconv, nn.Tanh()]\n","            model = down + [submodule] + up\n","        elif innermost:\n","            upconv = nn.ConvTranspose2d(\n","                inner_nc, outer_nc, kernel_size=4,\n","                stride=2, padding=1, bias=use_bias\n","            )\n","            down = [downrelu, downconv]\n","            up = [uprelu, upconv, upnorm]\n","            model = down + up\n","        else:\n","            upconv = nn.ConvTranspose2d(\n","                inner_nc * 2, outer_nc, kernel_size=4,\n","                stride=2, padding=1, bias=use_bias\n","            )\n","            down = [downrelu, downconv, downnorm]\n","            up   = [uprelu, upconv, upnorm]\n","            if use_dropout:\n","                model = down + [submodule] + up + [nn.Dropout(0.5)]\n","            else:\n","                model = down + [submodule] + up\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        if self.outermost:\n","            return self.model(x)\n","        else:\n","            return torch.cat([x, self.model(x)], 1)"],"metadata":{"id":"_i9pfrfJ28f2","executionInfo":{"status":"ok","timestamp":1736092785806,"user_tz":-180,"elapsed":467,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class UnetGenerator(nn.Module):\n","    \"\"\"Original style U-Net with flexible in/out channels.\"\"\"\n","    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n","                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n","        super(UnetGenerator, self).__init__()\n","        # construct unet structure from innermost to outermost\n","        unet_block = UnetSkipConnectionBlock(\n","            ngf * 8, ngf * 8, input_nc=None,\n","            submodule=None, norm_layer=norm_layer, innermost=True\n","        )\n","        # add intermediate layers with 8 * ngf filters\n","        for _ in range(num_downs - 5):\n","            unet_block = UnetSkipConnectionBlock(\n","                ngf * 8, ngf * 8, input_nc=None,\n","                submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout\n","            )\n","        # gradually reduce the number of filters from ngf * 8 to ngf\n","        unet_block = UnetSkipConnectionBlock(\n","            ngf * 4, ngf * 8, input_nc=None,\n","            submodule=unet_block, norm_layer=norm_layer\n","        )\n","        unet_block = UnetSkipConnectionBlock(\n","            ngf * 2, ngf * 4, input_nc=None,\n","            submodule=unet_block, norm_layer=norm_layer\n","        )\n","        unet_block = UnetSkipConnectionBlock(\n","            ngf, ngf * 2, input_nc=None,\n","            submodule=unet_block, norm_layer=norm_layer\n","        )\n","        # outermost\n","        self.model = UnetSkipConnectionBlock(\n","            output_nc, ngf, input_nc=input_nc,\n","            submodule=unet_block, outermost=True,\n","            norm_layer=norm_layer\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"swY2P3Wo3AFy","executionInfo":{"status":"ok","timestamp":1736092789806,"user_tz":-180,"elapsed":606,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["## 4) Partial Weight Loading"],"metadata":{"id":"Qx1YrZwD3CRR"}},{"cell_type":"code","source":["def create_pretrained_unet_for_manga(\n","    anime2sketch_ckpt=\"weights/netG.pth\",\n","    norm_layer=None,\n","    num_downs=8,\n","    ngf=64,\n","    use_dropout=False\n","):\n","    \"\"\"\n","    Create a new U-Net generator with input=1, output=3\n","    and partially load from the anime2sketch checkpoint (which is 3->1).\n","    \"\"\"\n","    if norm_layer is None:\n","        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n","\n","    # 1 input channel -> 3 output channels\n","    net_manga = UnetGenerator(\n","        input_nc=1,\n","        output_nc=3,\n","        num_downs=num_downs,\n","        ngf=ngf,\n","        norm_layer=norm_layer,\n","        use_dropout=use_dropout\n","    )\n","\n","    # Load the checkpoint\n","    ckpt = torch.load(anime2sketch_ckpt, map_location=\"cpu\")\n","    # Remove any 'module.' prefixes\n","    for key in list(ckpt.keys()):\n","        if \"module.\" in key:\n","            ckpt[key.replace(\"module.\", \"\")] = ckpt[key]\n","            del ckpt[key]\n","\n","    # Only load matching layers\n","    model_dict = net_manga.state_dict()\n","    pretrained_dict = {}\n","    for k, v in ckpt.items():\n","        if k in model_dict and model_dict[k].shape == v.shape:\n","            pretrained_dict[k] = v\n","\n","    model_dict.update(pretrained_dict)\n","    net_manga.load_state_dict(model_dict)\n","\n","    return net_manga"],"metadata":{"id":"EQ9y-44v3HSP","executionInfo":{"status":"ok","timestamp":1736092794073,"user_tz":-180,"elapsed":403,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["## 5) Create model, loss, optimizer"],"metadata":{"id":"GeXG6RcV3Isa"}},{"cell_type":"code","source":["anime2sketch_ckpt_path = \"/content/weights/netG.pth\"\n","\n","net_manga = create_pretrained_unet_for_manga(\n","    anime2sketch_ckpt=anime2sketch_ckpt_path,\n","    norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False),\n","    num_downs=8,\n","    ngf=64,\n","    use_dropout=False\n",")\n","net_manga.to(device)\n","net_manga.train()\n","\n","optimizer = torch.optim.Adam(net_manga.parameters(), lr=1e-4)\n","criterion = nn.L1Loss()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6d1Z8ga3RWq","executionInfo":{"status":"ok","timestamp":1736092936143,"user_tz":-180,"elapsed":1452,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"4a89ae5b-0e47-432d-9f80-72ea08cf96cc"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-28-2d18d22b2f53>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(anime2sketch_ckpt, map_location=\"cpu\")\n"]}]},{"cell_type":"markdown","source":["## 6) Training + Validation + Visualization"],"metadata":{"id":"AZE4uBTd3VQy"}},{"cell_type":"code","source":["def denormalize_lab_tensors(l_in, l_diff, a_pred, b_pred):\n","    \"\"\"\n","    Convert network outputs (normalized L-diff, A, B) back to an RGB image.\n","    Steps:\n","      1) final L = l_in + l_diff\n","      2) Scale back to Lab domain\n","      3) Convert Lab->RGB\n","    \"\"\"\n","    # l_in in [0,1]\n","    # l_diff in [-1,1]\n","    # final L in [0,1] (some potential for going out of range if the diff is big, but typically in [-1..1].)\n","    L_final = (l_in + l_diff).clamp(0,1)  # shape [H,W]\n","\n","    # A in [-1..1], B in [-1..1]\n","    # We map them back: A' = A*128, B' = B*128\n","    A_lab = a_pred * 128.0\n","    B_lab = b_pred * 128.0\n","\n","    # L_final is in [0..1], for lab we want [0..100].\n","    L_lab = L_final * 100.0\n","\n","    # Combine into [H,W,3]\n","    lab_img = torch.stack([L_lab, A_lab, B_lab], dim=-1).cpu().numpy()\n","    # Convert LAB -> RGB with skimage\n","    rgb_img = lab2rgb(lab_img.astype(np.float64))\n","    return rgb_img\n","\n","\n","def visualize_validation_samples(net, val_loader, num_samples=3, device=device):\n","    \"\"\"\n","    Grab a few batches from validation, display predictions vs ground truth.\n","    \"\"\"\n","    net.eval()\n","    with torch.no_grad():\n","        # Just take one batch\n","        for (l_in, l_diff_gt, a_gt, b_gt) in val_loader:\n","            # Move to GPU if available\n","            l_in     = l_in.to(device)\n","            l_diff_gt= l_diff_gt.to(device)\n","            a_gt     = a_gt.to(device)\n","            b_gt     = b_gt.to(device)\n","\n","            # Forward pass\n","            output = net(l_in)\n","            # output has shape [B,3,H,W]\n","            pred_l_diff = output[:,0:1,...]\n","            pred_a      = output[:,1:2,...]\n","            pred_b      = output[:,2:3,...]\n","\n","            # We'll visualize up to num_samples from this batch\n","            batch_size = l_in.size(0)\n","            n_show = min(batch_size, num_samples)\n","\n","            for i in range(n_show):\n","                # Extract single image\n","                l_in_i     = l_in[i,0,:,:].detach()     # [H,W]\n","                l_diff_i   = pred_l_diff[i,0,:,:].detach()\n","                a_i        = pred_a[i,0,:,:].detach()\n","                b_i        = pred_b[i,0,:,:].detach()\n","\n","                # Ground truth Lab\n","                gt_l_diff_i = l_diff_gt[i,0,:,:].detach()\n","                gt_a_i      = a_gt[i,0,:,:].detach()\n","                gt_b_i      = b_gt[i,0,:,:].detach()\n","\n","                # Convert predictions to RGB\n","                pred_rgb = denormalize_lab_tensors(l_in_i, l_diff_i, a_i, b_i)\n","                # Convert ground truth to RGB\n","                gt_rgb   = denormalize_lab_tensors(l_in_i, gt_l_diff_i, gt_a_i, gt_b_i)\n","\n","                # Prepare for plotting\n","                # We'll plot:\n","                #   (1) the \"input\" grayscale thresholded (for display),\n","                #   (2) predicted colorization,\n","                #   (3) ground truth\n","                # They are all in numpy HxWx3 or HxW\n","                inp_vis = l_in_i.cpu().numpy()  # [H,W], in [0..1]\n","\n","                fig, axes = plt.subplots(1,3, figsize=(12,4))\n","                # 1) input\n","                axes[0].imshow(inp_vis, cmap='gray', vmin=0, vmax=1)\n","                axes[0].set_title(\"Input L (thresholded)\")\n","\n","                # 2) prediction\n","                axes[1].imshow(pred_rgb)\n","                axes[1].set_title(\"Prediction (RGB)\")\n","\n","                # 3) ground truth\n","                axes[2].imshow(gt_rgb)\n","                axes[2].set_title(\"Ground Truth (RGB)\")\n","\n","                for ax in axes:\n","                    ax.axis('off')\n","                plt.tight_layout()\n","                plt.show()\n","\n","            break  # only visualize one batch\n","    net.train()\n","\n","\n","def run_validation(net, val_loader, criterion, device=device):\n","    \"\"\"\n","    Compute average validation L1 loss over the entire val_loader.\n","    \"\"\"\n","    net.eval()\n","    val_loss = 0.0\n","    count = 0\n","    with torch.no_grad():\n","        for (l_in, l_diff_gt, a_gt, b_gt) in val_loader:\n","            l_in     = l_in.to(device)\n","            l_diff_gt= l_diff_gt.to(device)\n","            a_gt     = a_gt.to(device)\n","            b_gt     = b_gt.to(device)\n","\n","            output = net(l_in)\n","            pred_l_diff = output[:,0:1,...]\n","            pred_a      = output[:,1:2,...]\n","            pred_b      = output[:,2:3,...]\n","\n","            loss_l_diff = criterion(pred_l_diff, l_diff_gt)\n","            loss_a      = criterion(pred_a, a_gt)\n","            loss_b      = criterion(pred_b, b_gt)\n","\n","            loss = loss_l_diff + loss_a + loss_b\n","            val_loss += loss.item() * l_in.size(0)\n","            count    += l_in.size(0)\n","\n","    avg_val_loss = val_loss / count\n","    net.train()\n","    return avg_val_loss"],"metadata":{"id":"LwlEnHgR3bK8","executionInfo":{"status":"ok","timestamp":1736092957458,"user_tz":-180,"elapsed":441,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["num_epochs = 20\n","print_every = 3  # visualize & show val loss every X epochs\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    total_samples = 0\n","\n","    # TQDM progress bar over training\n","    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n","    for (l_in, l_diff_gt, a_gt, b_gt) in pbar:\n","        l_in     = l_in.to(device)\n","        l_diff_gt= l_diff_gt.to(device)\n","        a_gt     = a_gt.to(device)\n","        b_gt     = b_gt.to(device)\n","\n","        optimizer.zero_grad()\n","        output = net_manga(l_in)\n","        pred_l_diff = output[:,0:1,...]\n","        pred_a      = output[:,1:2,...]\n","        pred_b      = output[:,2:3,...]\n","\n","        loss_l_diff = criterion(pred_l_diff, l_diff_gt)\n","        loss_a      = criterion(pred_a, a_gt)\n","        loss_b      = criterion(pred_b, b_gt)\n","\n","        lambda_l_diff = 1   # weight for the l_diff loss\n","        loss = lambda_l_diff * loss_l_diff + loss_a + loss_b\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * l_in.size(0)\n","        total_samples += l_in.size(0)\n","        pbar.set_postfix({\"loss\": f\"{(running_loss/total_samples):.4f}\"})\n","\n","    # End of epoch\n","    train_epoch_loss = running_loss / total_samples\n","    print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_epoch_loss:.4f}\")\n","\n","    # Every 3 epochs (or as desired), run validation and visualize\n","    if (epoch+1) % print_every == 0:\n","        val_loss = run_validation(net_manga, val_loader, criterion, device=device)\n","        print(f\"[Epoch {epoch+1}/{num_epochs}] Validation Loss: {val_loss:.4f}\")\n","\n","        # Visualize some predictions on val set\n","        visualize_validation_samples(net_manga, val_loader, num_samples=2, device=device)\n","\n","\n","# Save the fine-tuned model\n","save_path = \"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/Anime2Sketch_colorization/anime2sketch_finetuned.pth\"\n","model_dir = os.path.dirname(save_path)\n","os.makedirs(model_dir, exist_ok=True)\n","torch.save(net_manga, save_path)\n","print(\"Finished training! Model saved to:\", save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1rF9hCbGB18i7Kp7p-8VoqilgdBg_rZQD"},"id":"FFOHyFb-3g6n","executionInfo":{"status":"error","timestamp":1736094968418,"user_tz":-180,"elapsed":2004846,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"d542ba19-f7ff-4d4d-de04-f034e3e52e09"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["save_path = \"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/Anime2Sketch_colorization/anime2sketch_finetuned.pth\"\n","model_dir = os.path.dirname(save_path)\n","os.makedirs(model_dir, exist_ok=True)\n","torch.save(net_manga, save_path)\n","print(\"Finished training! Model saved to:\", save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OqIhZcdOVEf5","executionInfo":{"status":"ok","timestamp":1736095064436,"user_tz":-180,"elapsed":1377,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"c423974b-5ffb-4957-c1cd-30664b9a9e6e"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished training! Model saved to: /content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/Anime2Sketch_colorization/anime2sketch_finetuned.pth\n"]}]},{"cell_type":"markdown","source":["## Training with higher resolution"],"metadata":{"id":"sYx6SaB0VHKh"}},{"cell_type":"code","source":["anime2sketch_ckpt_path = \"/content/weights/netG.pth\"\n","\n","net_manga1 = create_pretrained_unet_for_manga(\n","    anime2sketch_ckpt=anime2sketch_ckpt_path,\n","    norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False),\n","    num_downs=8,\n","    ngf=64,\n","    use_dropout=False\n",")\n","net_manga1.to(device)\n","net_manga1.train()\n","\n","optimizer = torch.optim.Adam(net_manga1.parameters(), lr=1e-4)\n","criterion = nn.L1Loss()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCjfWeoFVk4H","executionInfo":{"status":"ok","timestamp":1736095399383,"user_tz":-180,"elapsed":1516,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"155c0034-66db-407f-ed78-1285b115f4dc"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-28-2d18d22b2f53>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(anime2sketch_ckpt, map_location=\"cpu\")\n"]}]},{"cell_type":"code","source":["num_epochs = 20\n","print_every = 3  # visualize & show val loss every X epochs\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    total_samples = 0\n","\n","    # TQDM progress bar over training\n","    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n","    for (l_in, l_diff_gt, a_gt, b_gt) in pbar:\n","        l_in     = l_in.to(device)\n","        l_diff_gt= l_diff_gt.to(device)\n","        a_gt     = a_gt.to(device)\n","        b_gt     = b_gt.to(device)\n","\n","        optimizer.zero_grad()\n","        output = net_manga1(l_in)\n","        pred_l_diff = output[:,0:1,...]\n","        pred_a      = output[:,1:2,...]\n","        pred_b      = output[:,2:3,...]\n","\n","        loss_l_diff = criterion(pred_l_diff, l_diff_gt)\n","        loss_a      = criterion(pred_a, a_gt)\n","        loss_b      = criterion(pred_b, b_gt)\n","\n","        lambda_l_diff = 0.5   # weight for the l_diff loss\n","        loss = lambda_l_diff * loss_l_diff + loss_a + loss_b\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * l_in.size(0)\n","        total_samples += l_in.size(0)\n","        pbar.set_postfix({\"loss\": f\"{(running_loss/total_samples):.4f}\"})\n","\n","    # End of epoch\n","    train_epoch_loss = running_loss / total_samples\n","    print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_epoch_loss:.4f}\")\n","\n","\n","    val_loss = run_validation(net_manga1, val_loader, criterion, device=device)\n","    print(f\"[Epoch {epoch+1}/{num_epochs}] Validation Loss: {val_loss:.4f}\")\n","\n","    # Every 3 epochs (or as desired), run validation and visualize\n","    if (epoch+1) % print_every == 0:\n","        # Visualize some predictions on val set\n","        visualize_validation_samples(net_manga1, val_loader, num_samples=2, device=device)\n","\n","\n","# Save the fine-tuned model\n","save_path = \"/content/drive/MyDrive/Okul/Eğitim/Ders/5. Dönem/YZV 303E - Deep Learning/Project/models/Anime2Sketch_colorization/anime2sketch_finetuned_512p.pth\"\n","model_dir = os.path.dirname(save_path)\n","os.makedirs(model_dir, exist_ok=True)\n","torch.save(net_manga1, save_path)\n","print(\"Finished training! Model saved to:\", save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1dRfH1wYDHplVDEZdWhAWsyLLYgAEJjRs"},"id":"41hyKkShVwDF","executionInfo":{"status":"ok","timestamp":1736099176077,"user_tz":-180,"elapsed":3764837,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}},"outputId":"70161cb3-2751-4b0f-dfaa-ee8ce68ff474"},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## 7) Inference Function"],"metadata":{"id":"SoOS0nJs33-A"}},{"cell_type":"code","source":["def colorize_manga_page(model, image_path, threshold=100, img_size=512, device=device):\n","    \"\"\"\n","    Let the user input their own manga page (uncolored),\n","    The model predicts L-diff, A, B -> return colorized RGB.\n","    \"\"\"\n","    model.eval()\n","    with torch.no_grad():\n","        bgr_img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","        if bgr_img is None:\n","            raise ValueError(f\"Could not load image at {image_path}\")\n","\n","        # Resize if needed\n","        if img_size is not None:\n","            bgr_img = cv2.resize(bgr_img, (img_size, img_size))\n","\n","        gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n","        l_input = np.where(gray_img >= threshold, 255, gray_img).astype(np.float32)\n","        l_in_norm = l_input / 255.0  # [0..1]\n","        l_in_torch = torch.tensor(l_in_norm, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n","\n","        # Forward\n","        output = model(l_in_torch)\n","        pred_l_diff = output[:,0:1,...]\n","        pred_a      = output[:,1:2,...]\n","        pred_b      = output[:,2:3,...]\n","\n","        # Convert to final RGB\n","        l_in_i = l_in_torch[0,0,:,:]\n","        colorized_rgb = denormalize_lab_tensors(l_in_i, pred_l_diff[0,0,:,:], pred_a[0,0,:,:], pred_b[0,0,:,:])\n","\n","    return colorized_rgb"],"metadata":{"id":"6BMQw7nc35tz","executionInfo":{"status":"ok","timestamp":1736099373268,"user_tz":-180,"elapsed":457,"user":{"displayName":"Alihan Esen","userId":"14538701485068487647"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# Example usage (after training):\n","colorized_image = colorize_manga_page(net_manga1, \"/content/test_images/Bleach v1-063.jpg\")\n","plt.imshow(colorized_image)\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"sLCPCsD6lHpP"},"execution_count":null,"outputs":[]}]}